---
title: "Resúmenes"
#subtitle: "11 al 12 de Noviembre 2024"
#description: "Auditorio Departamento de Matemática y Ciencia de la Computación"
title-block-banner: "#d394f8"
lang: es
format: 
  html:
    embed-resources: true
    anchor-sections: true
toc: true
toc-depth: 2
toc-location: left
toc-title: "Títulos de los trabajos"
---

# Sesión invitada

## Modelos estadísticos muestreados en tiempos aleatorios
**Soledad Torres Díaz**
*Universidad Central*
[Slides](../presentations/Soledad_Torres.pdf){target="_blank"}

En esta charla, presentamos un modelo de regresión muestreado en tiempos aleatorios, considerando que las variables pueden no ser independientes. El ruido utilizado en nuestro modelo se generaliza más allá del ruido blanco. A continuación, analizamos la consistencia del estimador por Mínimos Cuadrados para los parámetros del modelo. Finalmente, presentaremos simulaciones que ilustran el comportamiento del modelo y la efectividad de los estimadores en diversas condiciones. Estas simulaciones nos permitirán visualizar cómo las características del ruido influyen en la precisión de las estimaciones.

---

## Problemáticas con Salud una mirada a través de la Ciencia de Datos: Proyecto FONDEF ID23I10401 y otros
**Héctor Araya Carvajal**
*Universidad Adolfo Ibáñez*
[Slides](../presentations/Hector_Araya.pdf){target="_blank"}

En esta charla, exploraremos de manera muy general el contexto en el cual se desarrolla el proyecto Fondef ID23I10401, cuáles son los desafíos y lo que se desea lograr. Posteriormente, de ser posible se abordará más técnicamente un problema relacionado a complicaciones respiratorias en UCI.

---

## Data privacy and Bayesian inference
**Felipe Barrientos** 
*Florida State University, EEUU*

Protecting individuals' private information while still allowing modelers to draw inferences from confidential data sets is a concern of many data producers. Differential privacy is a framework that enables statistical analyses while controlling the potential leakage of private information. A significant challenge in statistical analysis for data privacy is to account for all sources of randomness, including that required to achieve specific levels of privacy. In this talk, we combine Bayesian and differential privacy approaches to develop methods that perform statistical inference while bounding the potential leakage of information. We will discuss different approaches for different statistical data privacy, including applications to real data sets


---

## Model Risk 
**Daniela González Oberreuter & Yorka Romero**
*Banco Santander*

En Santander los modelos estadísticos son una herramienta esencial, ya que se utilizan como soporte de los procesos de decisión, para la generación de informes financieros y regulatorios, así como para proveer información predictiva en distintas funciones de gestión de riesgo. No obstante, su uso conlleva riesgos potenciales y además dada su relevancia, es importante establecer un marco adecuado para gestionar dichos riesgos.

El riesgo de modelo se define como la materialización de las potenciales consecuencias negativas derivadas de las decisiones basadas en resultados de modelos incorrectos, inadecuados o utilizados de forma indebida. En esta presentación, profundizaremos en los distintos tipos de riesgos que se miden en el sector bancario y analizaremos el ciclo de vida de los modelos utilizados, con el fin de garantizar su uso adecuado y minimizar los riesgos asociados.

---

# Sesiones orales

## Partial identification in ILSA studies of educational achievement: A strategy for producing credible interval estimates with student non-participation  
**Maximiliano Romero**  
*International Association for the Evaluation of Educational Achievement (IEA)*

Este trabajo aborda el problema de la no participación estudiantil en los estudios internacionales de evaluación a gran escala (ILSA), que afecta la estimación de parámetros poblacionales como el rendimiento promedio. Tradicionalmente, estos estudios suponen que la no participación es ignorada, lo que permite obtener estimaciones puntuales de los parámetros. Sin embargo, este enfoque depende de una fuerte suposición de independencia entre el rendimiento de los estudiantes y su probabilidad de participar, lo que puede no ser realista y comprometer la credibilidad de los resultados.

El enfoque propuesto introduce la identificación parcial para producir estimaciones intervalares más creíbles bajo supuestos menos restrictivos. La identificación parcial permite generar intervalos que reflejan tanto el error muestral como la incertidumbre estructural provocada por la falta de datos de los estudiantes no participantes. Esto se logra sin imponer suposiciones fuertes sobre el rendimiento de los no participantes, evitando la necesidad de depender únicamente de los datos observados.

Utilizando datos del International Computer and Information Literacy Study (ICILS 2018), se ilustran diferentes escenarios que muestran cómo varían los intervalos de estimación del rendimiento promedio según los supuestos sobre la no participación. El enfoque permite ajustar la amplitud de los intervalos según el grado de credibilidad de las suposiciones impuestas, mejorando la robustez de los resultados en comparación con los enfoques tradicionales que eliminan la ambigüedad, pero a costa de la credibilidad.

El artículo ofrece una metodología para manejar la incertidumbre no muestral y propone la combinación de técnicas de muestreo con análisis de identificación parcial para obtener intervalos creíbles. Este enfoque proporciona una herramienta valiosa en el análisis de estudios como PISA y TIMSS, donde la no participación es un problema recurrente y puede introducir sesgos significativos en la estimación de parámetros clave.

La identificación parcial permite explorar una serie de suposiciones intermedias entre los extremos de ignorabilidad total y mínimas restricciones, contribuyendo a una estimación más robusta y creíble de los parámetros poblacionales. Este trabajo destaca cómo el uso de intervalos de identificación parcial puede mejorar la interpretación de los resultados en estudios con tasas de participación subóptimas.

---

## Factores asociados en el rendimiento de pruebas Simce: propuesta de análisis inferencial y hallazgos  
**Esteban Avarca Oviedo**  
*Universidad de Santiago de Chile (USACH)*
[Slides](../presentations/Esteban_Avarca.pdf){target="_blank"}


La Agencia de Calidad de la Educación desarrolló una herramienta con el propósito de estudiar y analizar cómo distintos aspectos de la experiencia escolar se asocian con los resultados educativos de la prueba Simce y de los Indicadores de desarrollo personal y social (IDPS). Estos son los denominados “factores asociados” que refieren sobre variadas temáticas educativas y son evaluados por medio de los Cuestionarios de Calidad Simce.

El desarrollo de estos factores se realiza en etapas, primero recopilando la información mediante la aplicación del Simce, que incluye tanto las pruebas de conocimiento como los resultados de los Cuestionarios de Calidad y Contexto de la Educación (CCCE) y a través de la información proporcionada por el Ministerio de Educación.

En una segunda etapa, se definen las variables dependientes (puntajes SIMCE y resultados IDPS) e independientes. La construcción de los factores asociados como variables independientes desempeña un papel crucial en esta metodología y sigue un proceso específico. En primer lugar, se definen teóricamente las variables que se espera que estén asociadas con Simce y que permitan ofrecer recomendaciones respecto a la gestión educativa. Posteriormente, este constructo se elabora mediante un conjunto de ítems de los CCCE, y su cuantificación se realiza mediante un análisis factorial. Además, es importante destacar que los factores asociados medidos son para distintos actores que completaron los Cuestionarios, tales como estudiantes, padres o apoderados, docentes y directores.

La tercera etapa contempla la estrategia de análisis, para ello se realiza un Modelamiento estadístico utilizando las variables explicadas y explicativas. El modelo utilizado corresponde a un modelo lineal jerárquico que permite distinguir la variabilidad entre los distintos niveles de análisis, en este caso estudiantes y establecimientos educativos. La selección de las variables que se incluyen en cada modelo es determinada desde una revisión teórica. Es decir, se seleccionan las variables que se espera desde la teoría tengan una asociación con Simce e IDPS. Luego, algunas de estas variables se pueden descartar por varios aspectos, el primero de ellos es por multicolinealidad, otro aspecto a considerar para descartar variables corresponde a la significancia y signo esperado, donde posterior al modelamiento se verifica qué variables son significativas en el modelo y tengan un signo esperado teóricamente. Luego, para las variables que resultan del modelo final se obtiene una magnitud de la asociación. Esta magnitud es obtenida con la siguiente metodología: (1) Para cada factor asociado se generan cuatro grupos, definidos por los cuartiles de la variable, (2) Se obtiene para cada grupo el valor esperado promedio del grupo utilizando las estimaciones del modelo y (3) Se realiza la diferencia del grupo con mejores y peores resultados en términos del Simce esperado, obteniendo así la magnitud de la estimación. Esta magnitud se utiliza como una referencia de las diferencias que se pueden generar en Simce al tener mejores resultados en un Factor Asociado.

Utilizando la metodología antes descrita, se estimó que factores como “Ambiente protegido”, “Mentalidad de crecimiento” y “Expectativas de padres y madres” tienen un “impacto” relevante en los resultados académicos.

---

## Modelo BNP para datos discretos con aplicación al rendimiento de clubes deportivos  
**Cristian Capetillo Constela**  
*Pontificia Universidad Católica de Chile (PUC)*
[Slides](../presentations/Cristian_Capetillo.pdf){target="_blank"}

En toda área del conocimiento existe un interés particular en los datos del tipo discreto. Se podrían mencionar fácilmente datos como la frecuencia de eventos sísmicos, el número de productos vendidos por una tienda, el número de cigarrillos fumados por persona y el número de automóviles en una intersección, cada uno relacionado con la geología, la economía, la medicina o la planificación urbana, respectivamente.

En el contexto de los modelos paramétricos, el primer modelo para datos discretos, en particular de conteo, es el popular modelo de Poisson. Tal popularidad, lamentablemente, viene acompañada de su característica restrictiva de equidispersión. Alternativas al modelo de Poisson son la distribución Binomial-Negativa o versiones cero-infladas tales como los modelos ZIP y ZINB (véase, por ejemplo, Agresti, 2002). Sin embargo, la naturaleza restrictiva de los modelos paramétricos es bien conocida. Con un espacio de parámetros de dimensión finita, se podría caer en un problema de especificación. Más aún, un modelo paramétrico puede verse como caso particular de uno no paramétrico (Ghosal y van der Vaart, 2017).

La teoría Bayesiana No Paramétrica (BNP) está bien desarrollada en el contexto de variables aleatorias continuas. Para datos discretos, la afirmación puede ser al menos discutible. La incorporación de una variable subyacente continua, sin embargo, puede ayudar a transferir la teoría continua a una discreta. En este trabajo se desarrolla un modelo de regresión flexible y una metodología de selección de modelos para datos de tipo discreto utilizando el redondeo de kernels continuos (ver Canale y Dunson (2011)). En particular, se desarrolla un modelo LDDP redondeado, dotado de un esquema MCMC para un fácil cálculo a posteriori. El modelo se somete a un estudio de simulación y se aplica a un conjunto de datos correspondiente al desempeño de un equipo de fútbol a través de los años.

**Nota:** Las simulaciones iniciales no han sido exhaustivas. No obstante, los resultados preliminares sugieren un buen comportamiento asintótico del modelo, abriendo un camino prometedor para su desarrollo.

---

## Weibull random fields through Clayton spatial copula: an application to mining haul roads  
**Eloy Alvarado Narváez**  
*Universidad Técnica Federico Santa María (UTFSM)*
[Slides](../presentations/Eloy_Alvarado.pdf){target="_blank"}

The precise assessment and characterization of road surfaces are crucial for maintaining operational efficiency and ensuring the reliability of transportation networks. This is especially true for off-the-road surfaces in mining operations, where irregular terrains pose unique challenges that traditional evaluation methods often fail to capture. As a result, there is a need for more sophisticated approaches that can better model the complexity of these environments. 

In response to this challenge, we propose the construction of a Weibull random field using a spatial Clayton copula framework. This approach allows for greater flexibility by accommodating different types of asymmetries, making it particularly well-suited for the irregular patterns observed in mining haul roads. We investigate the second-order and geometrical properties of the proposed Weibull process and derive analytical expressions for the bivariate distribution. 

To further assess our model, we compare it with two alternative approaches: a Weibull random field obtained via a monotone transformation of a scaled χ² random process, and a model based on the classical Gaussian copula. The effectiveness of our methodology is demonstrated through the analysis of high-resolution imagery from Mina Sur in northern Chile, showing that the proposed model provides a more accurate representation of the spatial structure of haul road surfaces than traditional methods.

---

## Predicción de Desembarcos de Pulpo (Octopus vulgaris) con Inteligencia Artificial: Una Demo Interactiva en R  
**Victor Sanz-Fernández**  
*Pontificia Universidad Católica de Valparaíso (PUCV)*
[Slides](../presentations/Victor_Sanz.pptx){target="_blank"} [Código](../presentations/Victor_Sanz.R){target="_blank"}

La pesquería de pulpo (*Octopus vulgaris*) tiene una gran relevancia económica y social en el golfo de Cádiz, en el sur de España, y representa una actividad clave tanto para la economía local como para la Unión Europea. Poder prever los desembarques de esta especie es esencial para gestionar la pesquería de manera sostenible y tomar decisiones fundamentadas sobre su explotación. En este estudio, se emplean modelos de inteligencia artificial, específicamente redes neuronales autorregresivas, para capturar los patrones complejos y no lineales que caracterizan los desembarques de pulpo. Los datos de desembarques comerciales (primera venta) utilizados provienen del Sistema de Información Andaluz de Comercialización y Producción Pesquera (IDAPES) de Andalucía (España) y cubren el periodo desde enero de 2000 hasta diciembre de 2022 para las regiones suratlántica y mediterránea de Andalucía. Dado que los juveniles de esta especie se asientan entre septiembre y noviembre después del reclutamiento y el desove ocurre en verano en ambas regiones, los desembarques de octubre de un año hasta septiembre del año siguiente se consideran una cohorte. Así, el registro temporal comprende desde octubre de 2000 hasta septiembre de 2022. 

Durante la presentación en las III Jornadas de Ingeniería Estadística de la Universidad de Santiago de Chile (USACH), se realizará una demostración en vivo en R donde se aplicarán modelos de redes neuronales autorregresivas para proyectar los desembarques de pulpo en dos escenarios para ambas regiones: uno a corto plazo, correspondiente a 2023, y otro a largo plazo, que abarca el periodo 2023-2024. Para construir y calibrar estos modelos, se utilizó el 95% de los datos históricos (2000-2021), mientras que el 5% restante (2022) se reservó para una validación externa. En esta fase de construcción y calibración, se probaron seis configuraciones del parámetro de decaimiento del peso (decay: 0, 0.1, 0.2, 0.3, 0.4 y 0.5), seleccionando la mejor opción según el índice de persistencia (PI). La validación externa de los modelos se evaluó con diversos criterios de error: coeficiente de determinación ($R^2$), raíz cuadrada del error cuadrático medio (RMSE), error absoluto medio (MAE), coeficiente de eficiencia ($E_2$) y el índice de persistencia con desfase de un mes. 

---

## El uso de los modelos de distribución de especies para la construcción de distribuciones a priori en inferencia bayesiana  
**Mateo Antonio Morales Herrera**  
*Universidad de Chile (UChile)*

Este trabajo se sitúa en la intersección del modelamiento de nicho climático y la inferencia bayesiana, combinando dos enfoques importantes en ecología. El modelamiento de nicho climático se basa en el concepto de Hutchinson, que permite identificar áreas ambientales favorables o desfavorables de una especie, mientras que la inferencia bayesiana se utiliza para incorporar información externa en la prueba de hipótesis.

El objetivo principal es utilizar el modelamiento de nicho climático para generar distribuciones a priori informativas que comparen la proporción de sitios ocupados por especies exóticas en diferentes ambientes dentro de un procedimiento bayesiano (factor de Bayes). Para esto, se trabajará con cuatro especies de leguminosas exóticas en Chile Central. La investigación propone construir el nicho climático para realizar un muestreo aleatorio de espacios adecuados (1) y no adecuados (0) para las especies, obteniendo distribuciones a priori que permitan realizar una prueba de hipótesis.

En lugar de utilizar información a priori no informativa, común en estudios ecológicos, se usará información derivada del modelamiento de nicho. Estos datos binarios permitirán estimar distribuciones Beta, que son adecuadas para representar la proporción de sitios ocupados por las especies. Finalmente, la comparación entre ambientes se hará utilizando tanto información de campo como los modelos de nicho, empleando el factor de Bayes como estadístico de prueba.

---

## Estimación de Parámetros mediante Simulación-Extrapolación en Procesos Autorregresivos cercanos a la Raíz Unitaria  
**Pablo Vicari Ruiz**  
*Universidad de Santiago de Chile (USACH)*

En la actualidad, el análisis de series temporales ha adquirido una gran relevancia debido a su capacidad para modelar y predecir fenómenos en áreas tan diversas como la economía, la climatología y la epidemiología. En este contexto, los modelos autorregresivos (AR) juegan un papel crucial al permitir estimar la evolución futura de una variable a partir de sus valores pasados. El presente trabajo se enfoca en mejorar la precisión en la estimación del parámetro $\phi$ en procesos autorregresivos, con especial énfasis en escenarios cercanos a la raíz unitaria. Estos escenarios son fundamentales para entender la dependencia temporal en los modelos AR y los modelos autorregresivos irregulares (iAR), que son esenciales para capturar la dinámica de los datos en diversos campos del conocimiento.

Para abordar esta problemática, se implementó el algoritmo SIMEX (Simulación-Extrapolación), originalmente desarrollado en el contexto de regresión, pero cuya aplicación a modelos autorregresivos es un campo en exploración. En este trabajo, se aplicó SIMEX con el objetivo de corregir el sesgo causado por la cercanía a la raíz unitaria en la estimación del parámetro $\phi$. Se probaron múltiples combinaciones de parámetros con el fin de identificar la configuración óptima para dicha estimación. Además, el rendimiento de SIMEX fue comparado con los enfoques tradicionales de estimación, como Máxima Verosimilitud (MLE) y Mínimos Cuadrados Ordinarios (LSE).

